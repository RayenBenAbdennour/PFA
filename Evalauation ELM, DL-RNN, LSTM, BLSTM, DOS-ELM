# Calcul des métriques pour chaque modèle
for name, y_pred in [('ELM', y_pred_elm), ('LSTM', y_pred_lstm),
                     ('DL-RNN', y_pred_dl_rnn), ('BLSTM', y_pred_blstm),
                     ('DOS-ELM', y_pred_dos_elm)]:
    mse = mean_squared_error(true_rul, y_pred)
    rmse = np.sqrt(mse)
    mae = mean_absolute_error(true_rul, y_pred)
    r2 = r2_score(true_rul, y_pred)
    score = cmapss_score(true_rul, y_pred)
    results[name] = {'MSE': mse, 'RMSE': rmse, 'MAE': mae, 'R2': r2, 'Score': score}

# Afficher les résultats
results_df = pd.DataFrame(results).T
print("\nRésultats des métriques pour FD001:")
print(results_df.round(2))
import matplotlib.pyplot as plt

plt.figure(figsize=(12, 6))
plt.plot(true_rul, label='True RUL', marker='o')
plt.plot(y_pred_elm, label='ELM', marker='x')
plt.plot(y_pred_lstm, label='LSTM', marker='s')
plt.plot(y_pred_dl_rnn, label='DL-RNN', marker='d')
plt.plot(y_pred_blstm, label='BLSTM', marker='^')
plt.plot(y_pred_dos_elm, label='DOS-ELM', marker='*')
plt.xlabel('Moteur')
plt.ylabel('RUL (cycles)')
plt.title('Prédictions RUL pour FD001')
plt.legend()
plt.show()
plt.figure(figsize=(12, 6))
for name, y_pred in [('ELM', y_pred_elm), ('LSTM', y_pred_lstm),
                     ('DL-RNN', y_pred_dl_rnn), ('BLSTM', y_pred_blstm),
                     ('DOS-ELM', y_pred_dos_elm)]:
    errors = y_pred - true_rul
    plt.hist(errors, bins=30, alpha=0.3, label=name)
plt.xlabel('Erreur (Prédiction - Vrai RUL)')
plt.ylabel('Fréquence')
plt.title('Distribution des erreurs pour FD001')
plt.legend()
plt.show()
metrics = ['RMSE', 'MAE', 'MSE', 'R2', 'Score']
fig, axes = plt.subplots(1, 5, figsize=(20, 4))
for i, metric in enumerate(metrics):
    axes[i].bar(results_df.index, results_df[metric])
    axes[i].set_title(metric)
    axes[i].set_xticklabels(results_df.index, rotation=45)
plt.tight_layout()
plt.show()
import pandas as pd
import numpy as np
from sklearn.metrics import mean_squared_error, mean_absolute_error, r2_score

# Fonction pour le Score C-MAPSS
def cmapss_score(y_true, y_pred):
    score = 0
    for i in range(len(y_true)):
        diff = y_pred[i] - y_true[i]
        if diff < 0:
            score += np.exp(-diff / 13) - 1
        else:
            score += np.exp(diff / 10) - 1
    return score

# Supposons que y_pred_elm, y_pred_lstm, etc. et true_rul sont disponibles
# Remplacer par vos prédictions réelles après exécution des modèles
# Ici, des valeurs simulées pour l'exemple
true_rul = np.random.rand(100) * 100  # À remplacer par les vraies RUL
y_pred_elm = true_rul + np.random.randn(100) * 15
y_pred_lstm = true_rul + np.random.randn(100) * 12
y_pred_dl_rnn = true_rul + np.random.randn(100) * 14
y_pred_blstm = true_rul + np.random.randn(100) * 10
y_pred_dos_elm = true_rul + np.random.randn(100) * 13

# Calculer les métriques
results = {}
for name, y_pred in [('ELM', y_pred_elm), ('LSTM', y_pred_lstm),
                     ('DL-RNN', y_pred_dl_rnn), ('BLSTM', y_pred_blstm),
                     ('DOS-ELM', y_pred_dos_elm)]:
    mse = mean_squared_error(true_rul, y_pred)
    rmse = np.sqrt(mse)
    mae = mean_absolute_error(true_rul, y_pred)
    r2 = r2_score(true_rul, y_pred)
    score = cmapss_score(true_rul, y_pred)
    results[name] = {'MSE': mse, 'RMSE': rmse, 'MAE': mae, 'R2': r2, 'Score': score}

# Créer le tableau
results_df = pd.DataFrame(results).T
results_df = results_df.round(2)

# Afficher le tableau
print("\nTableau comparatif des modèles pour FD001:")
print(results_df)

# Sauvegarder au format CSV (optionnel)
results_df.to_csv('model_comparison_fd001.csv')
import pandas as pd
import numpy as np
from sklearn.metrics import mean_squared_error, mean_absolute_error, r2_score

# Fonction pour le Score C-MAPSS
def cmapss_score(y_true, y_pred):
    score = 0
    for i in range(len(y_true)):
        diff = y_pred[i] - y_true[i]
        if diff < 0:
            score += np.exp(-diff / 13) - 1
        else:
            score += np.exp(diff / 10) - 1
    return score

# Supposons que y_pred_elm, y_pred_lstm, etc. et true_rul sont disponibles
# Remplacer par vos prédictions réelles après exécution des modèles
# Ici, des valeurs simulées pour l'exemple
true_rul = np.random.rand(100) * 100  # À remplacer par les vraies RUL
y_pred_elm = true_rul + np.random.randn(100) * 15
y_pred_lstm = true_rul + np.random.randn(100) * 12
y_pred_dl_rnn = true_rul + np.random.randn(100) * 14
y_pred_blstm = true_rul + np.random.randn(100) * 10
y_pred_dos_elm = true_rul + np.random.randn(100) * 13

# Calculer les métriques
results = {}
for name, y_pred in [('ELM', y_pred_elm), ('LSTM', y_pred_lstm),
                     ('DL-RNN', y_pred_dl_rnn), ('BLSTM', y_pred_blstm),
                     ('DOS-ELM', y_pred_dos_elm)]:
    mse = mean_squared_error(true_rul, y_pred)
    rmse = np.sqrt(mse)
    mae = mean_absolute_error(true_rul, y_pred)
    r2 = r2_score(true_rul, y_pred)
    score = cmapss_score(true_rul, y_pred)
    results[name] = {'MSE': mse, 'RMSE': rmse, 'MAE': mae, 'R2': r2, 'Score': score}

# Créer le tableau
results_df = pd.DataFrame(results).T
results_df = results_df.round(2)

# Afficher le tableau
print("\nTableau comparatif des modèles pour FD001:")
print(results_df)

# Sauvegarder au format CSV (optionnel)
results_df.to_csv('model_comparison_fd001.csv')
import pandas as pd
import numpy as np
from sklearn.metrics import mean_squared_error, mean_absolute_error, r2_score

# Fonction pour le Score C-MAPSS
def cmapss_score(y_true, y_pred):
    score = 0
    for i in range(len(y_true)):
        diff = y_pred[i] - y_true[i]
        if diff < 0:
            score += np.exp(-diff / 13) - 1
        else:
            score += np.exp(diff / 10) - 1
    return score

# Supposons que y_pred_elm, y_pred_lstm, etc. et true_rul sont disponibles
# Remplacer par vos prédictions réelles après exécution des modèles
# Ici, des valeurs simulées pour l'exemple
true_rul = np.random.rand(100) * 100  # À remplacer par les vraies RUL
y_pred_elm = true_rul + np.random.randn(100) * 15
y_pred_lstm = true_rul + np.random.randn(100) * 12
y_pred_dl_rnn = true_rul + np.random.randn(100) * 14
y_pred_blstm = true_rul + np.random.randn(100) * 10
y_pred_dos_elm = true_rul + np.random.randn(100) * 13

# Calculer les métriques
results = {}
for name, y_pred in [('ELM', y_pred_elm), ('LSTM', y_pred_lstm),
                     ('DL-RNN', y_pred_dl_rnn), ('BLSTM', y_pred_blstm),
                     ('DOS-ELM', y_pred_dos_elm)]:
    mse = mean_squared_error(true_rul, y_pred)
    rmse = np.sqrt(mse)
    mae = mean_absolute_error(true_rul, y_pred)
    r2 = r2_score(true_rul, y_pred)
    score = cmapss_score(true_rul, y_pred)
    results[name] = {'MSE': mse, 'RMSE': rmse, 'MAE': mae, 'R2': r2, 'Score': score}

# Créer le tableau
results_df = pd.DataFrame(results).T
results_df = results_df.round(2)

# Afficher le tableau
print("\nTableau comparatif des modèles pour FD001:")
print(results_df)

# Sauvegarder au format CSV (optionnel)
results_df.to_csv('model_comparison_fd001.csv')
import pandas as pd
import numpy as np
from sklearn.preprocessing import MinMaxScaler
from sklearn.metrics import mean_squared_error
import matplotlib.pyplot as plt

# Charger les données FD001
train_data = pd.read_csv('train_FD001.txt', sep='\s+', header=None)
test_data = pd.read_csv('test_FD001.txt', sep='\s+', header=None)
rul_data = pd.read_csv('RUL_FD001.txt', sep='\s+', header=None)

# Noms des colonnes
columns = ['unit_number', 'time_cycle'] + [f'sensor_{i}' for i in range(1, 25)]  # Changed to 25 to create 26 total columns

train_data.columns = columns
test_data.columns = columns
rul_data.columns = ['RUL']

# Calculer le RUL pour l'entraînement
def compute_rul(df):
    rul = []
    for unit in df['unit_number'].unique():
        unit_data = df[df['unit_number'] == unit]
        max_cycle = unit_data['time_cycle'].max()
        rul.append(max_cycle - unit_data['time_cycle'])
    return np.concatenate(rul)

train_data['RUL'] = compute_rul(train_data)
train_data['RUL'] = np.minimum(train_data['RUL'], 130)  # Piecewise RUL

# Sélection des capteurs
selected_sensors = ['sensor_2', 'sensor_3', 'sensor_4', 'sensor_7', 'sensor_8',
                    'sensor_9', 'sensor_11', 'sensor_12', 'sensor_13', 'sensor_14',
                    'sensor_15', 'sensor_17', 'sensor_20', 'sensor_21']

# Normalisation
scaler = MinMaxScaler()
train_data[selected_sensors] = scaler.fit_transform(train_data[selected_sensors])
test_data[selected_sensors] = scaler.transform(test_data[selected_sensors])

# Préparer les données
X_train = train_data[selected_sensors].values
y_train = train_data['RUL'].values
X_test = test_data.groupby('unit_number').last().reset_index()[selected_sensors].values
true_rul = rul_data['RUL'].values

# Implémentation simplifiée de l'ELM
class ELM:
    def __init__(self, n_hidden=500):
        self.n_hidden = n_hidden

    def fit(self, X, y):
        self.input_weights = np.random.randn(X.shape[1], self.n_hidden)
        self.bias = np.random.randn(self.n_hidden)
        H = np.tanh(X @ self.input_weights + self.bias)
        self.output_weights = np.linalg.pinv(H) @ y

    def predict(self, X):
        H = np.tanh(X @ self.input_weights + self.bias)
        return H @ self.output_weights

# Entraîner et prédire avec ELM
elm = ELM(n_hidden=500)
elm.fit(X_train, y_train)
y_pred_elm = elm.predict(X_test)

# Visualisation 1 : Comparaison True RUL vs Prédictions ELM
plt.figure(figsize=(10, 6))
plt.plot(true_rul, label='RUL réels', marker='o', linestyle='-', color='blue')
plt.plot(y_pred_elm, label='Prédictions ELM', marker='x', linestyle='--', color='red')
plt.xlabel('ID_Moteur')
plt.ylabel('RUL (cycles)')
plt.title('Comparaison RUL réels et Prédictions ELM pour FD001')
plt.legend()
plt.grid(True)
plt.show()

# Visualisation 2 : Histogramme des erreurs
errors = y_pred_elm - true_rul
plt.figure(figsize=(10, 6))
plt.hist(errors, bins=30, color='purple', alpha=0.7, edgecolor='black')
plt.xlabel('Erreur (Prédiction ELM - True RUL)')
plt.ylabel('Fréquence')
plt.title('Distribution des erreurs de prédiction ELM pour FD001')
plt.grid(True)
plt.show()

# Afficher le RMSE pour référence
rmse = np.sqrt(mean_squared_error(true_rul, y_pred_elm))
print(f"RMSE ELM: {rmse:.2f}")
import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
import seaborn as sns
from sklearn.preprocessing import MinMaxScaler
from sklearn.metrics import mean_squared_error, mean_absolute_error, r2_score
from sklearn.neural_network import MLPRegressor
from sklearn.model_selection import train_test_split

# Charger les données FD001
train_data = pd.read_csv('train_FD001.txt', sep=' ', header=None)

# Nettoyer les colonnes inutiles (colonnes avec NaN à la fin)
train_data = train_data.iloc[:, :-2]

# Prétraitement
scaler = MinMaxScaler()
train_features = train_data.iloc[:, 2:].values  # Ignorer ID et cycle
train_features_scaled = scaler.fit_transform(train_features)

# Préparer les étiquettes RUL pour l'entraînement
rul_train = np.maximum(0, 125 - train_data.groupby(0).cumcount())
X_train, X_val, y_train, y_val = train_test_split(train_features_scaled, rul_train, test_size=0.15, random_state=42)

# Entraîner le modèle ELM sur plusieurs runs pour capturer la variabilité
n_runs = 5
metrics_runs = {'RMSE': [], 'MSE': [], 'MAE': [], 'R2': []}

for run in range(n_runs):
    elm = MLPRegressor(hidden_layer_sizes=(500,), activation='relu', max_iter=1, solver='lbfgs', random_state=42+run)
    elm.fit(X_train, y_train)
    elm_val_pred = elm.predict(X_val)

    # Calcul des métriques pour chaque run
    metrics_runs['RMSE'].append(np.sqrt(mean_squared_error(y_val, elm_val_pred)))
    metrics_runs['MSE'].append(mean_squared_error(y_val, elm_val_pred))
    metrics_runs['MAE'].append(mean_absolute_error(y_val, elm_val_pred))
    metrics_runs['R2'].append(r2_score(y_val, elm_val_pred))

# Convertir les métriques en DataFrame pour faciliter la visualisation
metrics_df = pd.DataFrame(metrics_runs)

# Visualisation des métriques avec différents types de plots

# 1. Bar Plot pour la moyenne des métriques
plt.figure(figsize=(8, 6))
metrics_df.mean().plot(kind='bar', color=['skyblue', 'lightgreen', 'salmon', 'gold'])
plt.title('Moyenne des métriques pour ELM')
plt.ylabel('Valeur')
plt.xticks(rotation=45)
plt.tight_layout()
plt.show()

# 2. Box Plot pour la distribution des métriques
plt.figure(figsize=(8, 6))
sns.boxplot(data=metrics_df, palette='Set2')
plt.title('Distribution des métriques pour ELM (5 runs)')
plt.ylabel('Valeur')
plt.tight_layout()
plt.show()

# 3. Violin Plot pour une vue plus détaillée de la distribution
plt.figure(figsize=(8, 6))
sns.violinplot(data=metrics_df, palette='Set3')
plt.title('Distribution détaillée des métriques pour ELM (5 runs)')
plt.ylabel('Valeur')
plt.tight_layout()
plt.show()

# 4. Line Plot pour visualiser l'évolution des métriques sur les runs
plt.figure(figsize=(8, 6))
for metric in metrics_df.columns:
    plt.plot(metrics_df.index, metrics_df[metric], marker='o', label=metric)
plt.title('Évolution des métriques pour ELM sur 5 runs')
plt.xlabel('Run')
plt.ylabel('Valeur')
plt.legend()
plt.tight_layout()
plt.show()
